# 数仓

kylin

er图

es

前端埋点：采集的数据更细节，数据杂乱，有可能会丢

后端埋点：更可靠

前端埋点更多一点

canal

[atguigu@hadoop102 hadoop]$ vim core-site.xml 
[atguigu@hadoop102 hadoop]$ vim hdfs-site.xml 
[atguigu@hadoop102 hadoop]$ vim mapred-site.xml 
[atguigu@hadoop102 hadoop]$ vim yarn-site.xml 
[atguigu@hadoop102 hadoop]$ vim workers 

[atguigu@hadoop102 hadoop]$ vim mapred-site.xml 
[atguigu@hadoop102 hadoop]$ vim yarn-site.xml 

auto.offset.reset=

nohup 免挂断

& 守护进程 进入后台，

nohup & 后台免挂断

2020-05-13 09:25:28,753 INFO lifecycle.LifecycleSupervisor: Stopping component: PollableSourceRunner: { source:Taildir source: { positionFile: /opt/module/flume/taildir_position.json, skipToEnd: false, byteOffsetHeader: false, idleTimeout: 120000, writePosInterval: 3000 } counterGroup:{ name:null counters:{runner.backoffs.consecutive=196, runner.polls=197, runner.backoffs=196} } }
2020-05-13 09:25:28,753 WARN kafka.KafkaChannel: Sending events to Kafka failed
org.apache.kafka.common.errors.InterruptException: Flush interrupted.



### 1、概述

```
数据仓库是一个面向主题的，集成的，相对稳定的，反映历史变化的数据集合，用于支撑管理决策
```

### 2、框架概述

#### 技术选择

```
➢数据采集传输: Flume， Kafka， Sqoop ，Logstash，DataX,
➢数据存储: MySql, HDFS，HBase， Redis， MongoDB
➢数据计算: Hive，Tez，Spark， Flink， Storm
➢数据查询: Presto， Kylin， Impala， Druid
➢数据可视化: Echarts、 Superset、 QuickBI、DataV
➢任务调度: Azkaban、Oozie
➢集群监控: Zabbix
➢元数据管理: Atlas 
```

#### 版本型号

```
产品		   版本
Hadoop		3.1.3
Flume		1.9.0
Kafka		2.4.1
Hive		3.1.2
Sqoop		1.4.6
Java		1.8
Zookeeper	3.5.7
Presto		0.189
```

#### 服务器选择

```
物理机还是云主机
物理机特点：
1、安全性好
2、可自主控制
3、适合长期使用

云主机特点：
1、适应能力和部署能力强，客户在提交云主机租用请求后实时开通，立即获得服务
2、可快速实现业务扩容
3、短期使用服务价格稍低
```

### 3、集群服务器规划	

| 服务名称           | 子服务                | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ------------------ | --------------------- | --------------- | --------------- | --------------- |
| HDFS               | NameNode              | √               |                 |                 |
|                    | DataNode              | √               | √               | √               |
|                    | SecondaryNameNode     |                 |                 | √               |
| Yarn               | NodeManager           | √               | √               | √               |
|                    | Resourcemanager       |                 | √               |                 |
| Zookeeper          | Zookeeper Server      | √               | √               | √               |
| Flume(采集日志)    | Flume                 | √               | √               |                 |
| Kafka              | Kafka                 | √               | √               | √               |
| Flume（消费Kafka） | Flume                 |                 |                 | √               |
| Hive               | Hive                  | √               |                 |                 |
| MySQL              | MySQL                 | √               |                 |                 |
| Sqoop              | Sqoop                 | √               |                 |                 |
| Presto             | Coordinator           | √               |                 |                 |
|                    | Worker                |                 | √               | √               |
| Azkaban            | AzkabanWebServer      | √               |                 |                 |
|                    | AzkabanExecutorServer | √               |                 |                 |
| Druid              | Druid                 | √               | √               | √               |
| Kylin              |                       | √               |                 |                 |
| Hbase              | HMaster               | √               |                 |                 |
|                    | HRegionServer         | √               | √               | √               |
| Superset           |                       | √               |                 |                 |
| Atlas              |                       | √               |                 |                 |
| Solr               | Jar                   | √               |                 |                 |
| 服务数总计         |                       | 18              | 9               | 9               |

### 4、数据来源

#### 数据的类型

```
页面、事件、曝光、启动、错误
```



#### 数据的采集方式

```
代码埋点（前端/后端）、可视化埋点、全埋点
```

### 5、时间同步脚本

```
#!/bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo "========== $i =========="
    ssh -t $i "sudo date -s $1"
done
```

### 6、集群进程查看脚本

```
#! /bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done
```

### 7、hdfs多磁盘配置

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为file://${hadoop.tmp.dir}/dfs/data，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。

```
<property>
    <name>dfs.datanode.data.dir</name>
	<value>
		file:///dfs/data1,
		file:///hd2/dfs/data2,
		file:///hd3/dfs/data3,
		file:///hd4/dfs/data4
	</value>
</property>
```

### 8、集群数据均衡命令

#### 节点均衡

```
开启，start-balancer.sh -threshold 10 		各节点数据量相差不大于10%

停止，stop-balancer.sh

注意，很耗时间的事情
```

#### 磁盘均衡

```
1、生成均衡计划：hdfs diskbalancer -plan hadoop103
2、执行均衡计划：hdfs diskbalancer -execute hadoop103.plan.json
3、查看均衡任务：hdfs diskbalancer -query hadoop103
4、取消均衡计划：hdfs diskbalancer -cancel hadoop103.plan.json
```

### 9、LZO压缩配置

```
1、进入：cd /opt/module/hadoop-3.1.3/share/hadoop/common/
2、上传：编译好的jar包hadoop-lzo-0.4.20.jar
3、分发:xsync hadoop-lzo-0.4.20.jar
4、配置core-site.xml：
	<property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
5、分发配置：xsync core-site.xml
6、启动集群：start-dfs.sh 	start-yarn.sh
```

### 10、lzo手动创建索引

```
1、创建索引：
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo

2、测试：
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
```

### 11、性能测试

#### 写

```
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB
```

#### 读

```
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
```

#### 删除测试数据

```
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

### 12、参数调优

1、hdfs-site.xml
$$
dfs.namenode.handler.count = 20 * ln(cluster size)
$$

```
#The number of Namenode RPC server threads that listen to requests #from clients. If dfs.namenode.servicerpc-address is not #configured then Namenode RPC server threads listen to requests #from all nodes.
#NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发#的元数据操作。
#对于大集群或者有大量客户端的集群来说，通常需要增大参数#dfs.namenode.handler.count的默认值10。
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```

2、yarn-site.xml

```
1、yarn.nodemanager.resource.memory-mb
表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

2、yarn.scheduler.maximum-allocation-mb
单个任务可申请的最多物理内存量，默认是8192（MB）。
```

### 13、zookeeper群起脚本

```
#! /bin/bash

case $1 in
"start"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 启动 ------------
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 停止 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 状态 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac
```

### 14、kafka群起脚本

```
#! /bin/bash

case $1 in
"start"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
```

### 15、kafka常用命令

```
查看:bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list

创建：bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka  --create --replication-factor 1 --partitions 1 --topic first

删除：bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic first

生产：bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first

消费：bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first

详情：bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic first
```

### 16、kafka压测计算分区

生产测试kafka-producer-perf-test.sh

```
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
```

消费测试kafka-consumer-perf-test.sh

```
bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```

机器数量

```
Kafka机器数量（经验公式）=2*（峰值生产速度*副本数/100）+1		单位:Mb

比如我们的峰值生产速度是50M/s。副本数为2。
Kafka机器数量=2*（50*2/100）+ 1=3台
```

分区数量

```
分区数=Tt / min（Tp，Tc）
Tt:吞吐量
Tp:生产量
Tc:消费量

producer吞吐量=20mb/s；consumer吞吐量=50mb/s，期望吞吐量100mb/s；
分区数=100 / 20 =5分区
分区数一般设置为：3-10个
```

### 17、三种source对比

```
taildir source:断点续传，多目录

exec source：实时，数据可能会丢失

spooling directory source :实时，能断点续传
```

### 18、日志采集Flume配置

```
batchSize大小：Event 1K左右时，500-1000合适（默认为100）

采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

步骤：
1、vim /opt/module/flume/conf/file-flume-kafka.conf
#为各组件命名
a1.sources = r1
a1.channels = c1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json
a1.sources.r1.batchSize=500
#声明拦截器i1
#a1.sources.r1.interceptors =  i1
#a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.LogInterceptor$Builder

#描述channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic = topic_log
a1.channels.c1.parseAsFlumeEvent = false

#绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1

2、编写拦截器

3、打包上传拦截器

4、启动102和103上的flume开始生产数据到kafka
/opt/module/flume/bin/flume-ng agent --name a1 --conf /opt/module/flume/conf/file-flume-kafka.conf -Dflume.root.logger=info,console


日志采集启动脚本：
vim f1.sh

#! /bin/bash

case $1 in
"start"){
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1  &"
        done
};;	
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac

```

### 19、日志写入hdfs

```
1、配置文件编写
vim /opt/module/conf/test.xml
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=test


## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false


a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

2、启动flume向hdfs写数据
/opt/module/flume/bin/flume-ng agent --name a1 --conf-file /opt/module/flume/conf/test.xml -Dflume.root.logger=info,console
```

### 20、后台启动flume

收集业务数据f1.sh

```
vim f1.sh

#! /bin/bash

case $1 in
"start"){
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1  &"
        done
};;
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac

chmod +x f1.sh


```

### 将日志消费到hdfs

```
vim f2.sh

#! /bin/bash

case $1 in
"start"){
        for i in hadoop104
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log2.txt   2>&1 &"
        done
};;
"stop"){
        for i in hadoop104
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        done

};;
esac

chmod +x f2.sh
```



### 21、FileChannel

filechannel

```
速度快，agent进程挂掉数据会丢失，数据保存在JVM的堆内存
```

memorychannel

```
速度慢，agent进程挂掉数据会恢复
```

我们使用filechannel,因为也不是什么重要的数据

filechannel配置优化

```
1、通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。

2、checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据
```

HDFS Sink配置优化

```
基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0几个参数综合作用，效果如下：
（1）文件在达到128M时会滚动生成新文件
（2）文件创建超3600秒时会滚动生成新文件
```

### 22、flume内存优化

```
vim /opt/module/flume/conf/flume-env.sh
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"

JVM heap一般设置为4G或更高
-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。
-Xms表示JVM Heap(堆内存)最小尺寸，初始分配；-Xmx 表示JVM Heap(堆内存)最大允许的尺寸，按需分配。如果不设置一致，容易在初始化时，由于内存不够，频繁触发fullgc。
```

### 23、整个数据采集通道启停

```
#! /bin/bash

case $1 in
"start"){
	echo " -------- 启动 集群 -------"

	echo " -------- 启动 hadoop集群 -------"
	/opt/module/hadoop-3.1.3/sbin/start-dfs.sh 
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"

	#启动 Zookeeper集群
	zk.sh start

sleep 4s;

	#启动 Flume采集集群
	f1.sh start

	#启动 Kafka采集集群
	kf.sh start

sleep 6s;

	#启动 Flume消费集群
	f2.sh start

	};;
"stop"){
    echo " -------- 停止 集群 -------"


    #停止 Flume消费集群
	f2.sh stop

	#停止 Kafka采集集群
	kf.sh stop

    sleep 6s;

	#停止 Flume采集集群
	f1.sh stop

	#停止 Zookeeper集群
	zk.sh stop

	echo " -------- 停止 hadoop集群 -------"
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
	/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh 
};;
esac
```

### 24、sqoop

```
1、解压：tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/

2、改名：mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop

3、配置文件（/opt/module/sqoop/conf）
mv sqoop-env-template.sh sqoop-env.sh
vim sqoop-env.sh

export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
export HIVE_HOME=/opt/module/hive
export ZOOKEEPER_HOME=/opt/module/zookeeper
export ZOOCFGDIR=/opt/module/zookeeper/conf

4、拷贝MySQL的驱动：
cp /opt/software/mysql-connector-java-5.1.48.jar /opt/module/sqoop/lib/

5、验证：
bin/sqoop help
bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456

会出现一些命令和最后的数据库就可以了

6、使用:
sqoop import \
--connect jdbc:mysql://hadoop102:3306/dbtable \
--username root \
--password 123456 \
--target-dir /ouput \
--delete-target-dir \
--query "select * from user where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

```



### 25、造业务数据

```
1、mysql中新建数据库gmall
2、执行建表语句：gmall2020-04-01.sql
3、修改生成数据脚本的时间参数
4、执行生成数据脚本：java -jar gmall2020-mock-db-2020-04-01.jar
5、上传数据至hdfs:
vim gmall_mysql_to_hdfs.sh

#! /bin/bash
sqoop=/opt/module/sqoop/bin/sqoop
if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d '-1 day' +%F`
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 123456 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/gmall/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            final_total_amount, 
                            order_status, 
                            user_id, 
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            province_id,
                            benefit_reduce_amount,
                            original_total_amount,
                            feight_fee      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_activity_order(){
  import_data activity_order "select
                                id,
                                activity_id,
                                order_id,
                                create_time
                              from activity_order
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            name,
                            birthday,
                            gender,
                            email,
                            user_level, 
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              od.id,
                              order_id, 
                              user_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              od.create_time,
                              source_type,
                              source_id  
                            from order_detail od
                            join order_info oi
                            on od.order_id=oi.id
                            where DATE_FORMAT(od.create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                alipay_trade_no, 
                                total_amount,  
                                subject, 
                                payment_type, 
                                payment_time 
                              from payment_info 
                              where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              comment_txt,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                tm_id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time,
                        source_type,
                        source_id
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          spu_id,
                          tm_id,
                          category3_id,
                          limit_num,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1"
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "activity_order")
      import_activity_order
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;

"first")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_province
   import_base_region
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
esac

chmod +x gmall_mysql_to_hdfs.sh

执行业务数据上传脚本
```

### 26、造日志数据

```
1、进入造日志数据文件夹：cd applog

2、修改配置文件，更改时间参数：vim application.properties

3、分发配置

4、编写日志生成脚本(能够在两台虚拟机上产生日志数据)：vim lg.sh
#!/bin/bash
for i in hadoop102 hadoop103; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2020-04-01.jar >/dev/null 2>&1 &"
done

5、执行脚本生成日志数据:lg.sh

6、数据导入hdfs

```

