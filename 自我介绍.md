# 自我介绍

```
各位面试官好！

我叫张玉博，17年毕业于吉林大学，本科学历，三年以来一直从事IT行业，中途转岗到大数据开发，主要从事过大数据平台搭建，离线数仓搭建，离线指标分析，实时计算系统搭建，实时指标分析等。

最近在做的一个项目是直播平台的数仓搭建：这个项目主要包含以下三个方面：
数据仓库的搭建，
实时计算系统，
离线计算系统；

接下来我具体介绍以下这个项目：
该项目是一个高度定制化的直播平台项目，主要是数仓和离线指标这一块，数仓搭建时，和CTO商量后决定数仓使用5层架构，主要分为ods层,数据缓冲层，dwd基础数据层，dws数据轻度汇总，dwt数据重度汇总，ads数据应用层

分5从的主要作用是数据的隔离，避免部分数据的重复计算，同时也起到数据备份的作用；接下来我将介绍着5层主要做什么：

1、首先是ods层，ods层的数据主要来自于log日志和业务系统的数据，业务系统的数据从MySQL数据库中导入，log日志主要通过采集系统采集过来：
项目整体架构图：

2、首先需要介绍一下日志采集系统，采集系统的架构如下：
首先是由app和web进行埋点-》生成日志后发送到日志服务器-》之后通过Nginx进行一次负载-》进入到采集系统，这里我们使用三台机器作为我们的日志服务器；日志服务器的日志数据保存在相应磁盘上，保存30天

3、对于日志服务器上的数据采集，我们使用flume+kafka+flume的方式采集到hdfs的hive数仓中，选择flume的原因一是因为flume采集数据效果较好，另外对hdfs和kafka支持的很不错

选择flume时，主要从他的source，channel，sink进行技术选择，
	首先是source的选择，我们选择的是taildir source,因为他支持多目录和断点续传的功能，断点续传保证了flume挂了之后，数据不丢失，其原理是在底层维护了一个offset偏移量，1.6及以前这个偏移量需要手动维护，比较麻烦，另外taildir source不支持多级目录，需要手动递归遍历文件 + tail source解决
	
	其次是channel的选择，主要有memory channel基于内存，宕机会丢数据，性能很好，file channel基于磁盘，宕机不会丢数据，性能较差，适合安全性要求高的数据，kafka channel,主要是对接kafka时使用，通过节省sink组件提升效率
	flume这一层还做了一个拦截器，清除掉那些格式不合法的json，防止脏数据占用通道资源
	这一层还使用multipleing选择器将不同标签的数据分发到不同的topic中

4、下游数据传输使用的是kafka的消息队列，选择kafka的原因很简单，吞吐量大，为什么kafka块呢？一是因为顺序读写，二是因为多分区，三是因为采用零拷贝技术

	在kafka的架构包含productor broker consumer；一个broker可以包含多个topic，topic保存的数据包含多个partition，用来增加数据传输的并发度，broker和消费者信息都存储在zk中，这里zk主要负责：broker注册，leader选举，topic注册，consumer注册等

5、因为kafka性能比较高，在将数据接入数仓系统时，我们使用flume采集kafka通道内的数据，然后接入kafka中，在使用flume作为消费者时遇到一个小文件问题，不仅占用namenode的元数据内存，另外由于map任务会对每个小文件单独起任务，map任务过多会影响计算性能
最终通过配置flume的hdfssink来解决，主要调整了三个参数：rollinterval：按时间滚动生成一个文件，当时设置的是一个小时，防止单独以文件大小为滚动条件时最后一个较小的文件迟迟无法生成；rollsize：按文件大小滚动生成文件，设置为128M，考虑的是hdfs中文件块的大小是128M;rollcount:根据event数量滚动生成文件，设置为0，禁用

6、通过flume消费者，数据成功导入到hdfs上，到hdfs上时，直接将数据导入到hive中，导入方式采用的是azkaban进行调度，

7、接下来需要聊一下数仓的5层都分别作了什么：
ods数据缓冲层：
只设计了一个字段来存储，整体的数据存为一个大的json，方便数据的管理和备份
另外这一层使用snappy+parquet的压缩方式，使用这种方式的原因有3：一个是因为之前数据从flume出来时通过参数设置不会有小文件产生，另外后期数据量比较小的指标是通过spark sql来进行计算的，spark SQL对这种压缩方式都是支持很好的，最后就是这两种方式性能较好，比较快
这一层还做了数据的分区，按天进行分区，保证后面查询的过程中不需要进行全表的扫描，提升查询效率

还有业务数据时通过sqoop直接导入到ods层，这一块遇到一个hive和mysql的null值不匹配的问题，一个使用\N来表示空值，一个使用null来表示空值，后来通过加入null-string和null-non-string解决了这个问题
另外一个问题是sqoop导出数据的一致性问题，主要原因是sqoop底层默认是4个map，事务不是分布式的，这样某个任务挂了之后会导致最终不一致的情况出现，开始想通过暴力设置一个map来解决，但是觉得一定有其他办法，查阅官网找到了一个解决办法，就是设置staging-table这个参数指定中间临时表，它会等临时表成功后，在一个事务中将临时表中的数据导入到目标表中

dwd层基础数据层：
这一层需要对数据进行解析，维度建模，选择业务，确认粒度，确认维度，确认事实；之后是确定表关系，表的同步策略：全量同步（数据量不太大的，如），增量同步（数据量较大，周期变化，如订单详情，支付流水等），新增及变化（数据量大，缓慢变化，订单状态表，优惠券使用表，用户vip等级表），这部分做成拉链表，拉链表的制作过程：初始化拉链表t1,增加字段开始时间create_time，结束时间为update_time,9999-99-99，每天新增及变化表t2,t1 left join t2,最后union all t2

dws层数据轻度汇总：
每天各个维度的统计值：从用户看每日变化，如支付次数金额，浏览次数，停留时间，加收藏；从设备看每日变化：启动时间，活跃时间，渠道信息，设备信息等，从商品维度看的每日变化：商品的被支付，被评价，被收藏，被退款等；从活动角度看每日变化：创建时间，曝光时间，下单次数，下单金额，支付次数，支付金额，从地区角度看每日变化：地区编号，地区名称，下单次数，下单金额，支付次数，支付金额

dwt数据中度聚合：
如设备表的累积行为；会员表的累积行为，商品表的累积行为；活动表的累积行为；地区表的累积行为

ads层数据应用层：
根据设备的累积行为看：日活，周活，月活，每日新增，每周留存，沉默用户，回流用户，流失用户，连续三周活跃，7天连续三天活跃
根据会员的累积行为做：漏斗分析，从浏览人数，加购物车人数，下单人数，支付人数，大致比例在5%-10%，30%-40%，90%-95%
根据商品行为看：商品个数，商品销量topN，商品收藏topN,商品加购topN,商品的好评率，差评率等
根据活动累积表看：下单数目统计，支付数目统计，品牌复购率
根据地区累积表看：各个地区的活跃，下单，商品，活动情况

8、hive的元数据存放在MySQL中，使用atlas2.0做元数据管理，，集群使用zabbix监控，定时任务使用azkaban3.8.4做任务调度，权限管理使用ranger2.0，质量管理使用的是griffin,即席查询使用的是presto，因为支持的数据源比较多，多维分析使用的是kylin,因为有预计算，速度比较快，分析结果放入hbase中存储，hbase支持的数据量大，速度快，使用凤凰可以很方便的写SQL查询；可视化使用的supperset,

9、实时项目介绍：实施项目的日志采集系统使用的是离线采集系统那一套，而对于存放在MySQL中的业务数据，使用的是cannal实时监控，其原理是模仿MySQL中的slave的行为，读取binlog日志数据来实现跟踪变化数据，将变化数据生产到kafka中，使用spark streaming分析实时数据，其中对于重复数据，使用redis去重，使用hbase保存维度信息，使用Phoenix做明细查询，使用clickhouse做大宽表查询，使用es + kibana实现宽表查询和可视化，结果数据保存到MySQL中使用dataV实现可视化
```

